<!DOCTYPE html>
<!-- saved from url=(0032)http://www.cse.ust.hk/~lhouab/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lu Hou </title>
    <meta name="author" content="Lu Hou">

    <!-- Le styles -->
    <link href="./houlu_files/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="./houlu_files/font-awesome.min.css">
    <link href="./houlu_files/style.css" rel="stylesheet" type="text/css" media="all">
    <link rel="stylesheet" id="twentytwelve-style-css" target="_blank" href="./css/style2.css" type="text/css" media="all">
    <style type="text/css">
      body {
        padding-top: 30px;
        padding-bottom: 30px;
      }

      h3 {
        margin-top: 1.0em;
        margin-bottom: 0.3em;
        padding-bottom: 0.2em;
        line-height: 1.0;
        border-bottom: 1px solid #aaaaaa;
      }

      li {
        margin: 10px 0;
      }
    </style>


  </head>

  <body>

   

<div id="wrap">

<div class="container">

  <div class="content">


<div class="row">
  <div class="span14">


<div>



<div class="post-container">                
		<div class="post-thumb"><img src="./image/lulu.jpg" alt="protrait" width="180" style="margin-top:5px"></div>
		<div class="post-content">
			<h1 style="margin:-2px 0 0 0" class="civi_addr"> Lu Hou (侯璐)</h1>
					<p style="margin:-10px 0 0 0"class="civi_addr"> Researcher, Huawei Noah's Ark Lab </p>
					<p style="margin:-10px 0 0 0" class="civi_addr">Email: lhouab@connect.ust.hk</p>
					<p style="margin:3px 0 0 0" class="civi_addr">			
					I am a researcher at Huawei Noah's Ark Lab. I obtained my Ph.D. degree from <a href="https://www.ust.hk/"> Hong Kong University of Science and Technology </a> in 2019, under the supervision of Prof.<a href="https://www.cse.ust.hk/~jamesk/"> James T. Kwok</a>.  
Before that, I received my Bachelor's Degree from Qian Xuesen College (previously known as “College of Elite Education”) from Nanjing University of  Science and Technology </a> in 2014.</p>
	   <p style="margin:-10px 0 0 0" class="civi_addr">			
					   
	   </div>
</div>
		

<div style="padding:6px;"> </div>

<h3>Research Interests</h3>
<ul>
  <li>
    Compression and Acceleration of Deep Neural Networks</li>
  <li>
    Natural Language Processing</li>
  <li>
    Optimization in Deep Learning </li>
</ul>

<div style="padding:6px;"> </div>
<h3>Publications</h3>
<div>
<ol>
   <li>
    <p>
       <a>  Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark </a><br>
       Jiaxi Gu, Xiaojun Meng, Guansong Lu, <b>Lu Hou</b>, Minzhe Niu, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, Chunjing Xu, Hang Xu
       <i>To Appear in Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, Nov 2022. </i> <br>
    </p>
  </li>
	
  <li>
    <p>
       <a>  Towards efficient post-training quantization of pre-trained language models </a><br>
       Haoli Bai, <b>Lu Hou</b>, Lifeng Shang, Xin Jiang, Irwin King, Michael R Lyu<br>
       <i>To Appear in Proceedings of the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), Nov 2022. </i> <br>
    </p>
  </li>
	
   <li>
    <p>
       <a>  Compression of Generative Pre-trained Language Models via Quantization </a><br>
       Chaofan Tao, <b>Lu Hou</b>, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong<br>
       <i>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), <b>Outstanding Paper Award</b>, May 2022. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://openreview.net/pdf?id=8a9TvqmBNO7">  Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation </a><br>
       Wenliang Dai, <b>Lu Hou</b>, Lifeng Shang, Xin Jiang, Qun Liu, Pascale Fung<br>
       <i>Findings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL Findings), May 2022. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://openreview.net/pdf?id=cpDhcsEDC2"> FILIP: Fine-grained Interactive Language-Image Pre-Training </a><br>
       Lewei Yao*, Runhui Huang*, <b>Lu Hou*</b>, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, Chunjing Xu<br>
       <i>Proceedings of the The Tenth International Conference on Learning Representations (ICLR), Apr 2022. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://arxiv.org/pdf/2105.11144.pdf"> Improved OOD Generalization via Adversarial Training and Pretraining </a><br>
       Mingyang Yi, <b>Lu Hou</b>, Jiacheng Sun, Lifeng Shang, Xin Jiang,  Qun Liu, Zhiming Ma<br>
       <i>Proceedings of the Thirty-eighth International Conference on Machine Learning (ICML), Jul 2021. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://arxiv.org/pdf/2012.15701.pdf"> BinaryBERT: Pushing the Limit of BERT Quantization </a>[<a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BinaryBERT">code</a>]<br>
       Haoli Bai, Wei Zhang, <b>Lu Hou</b>, Lifeng Shang, Jing Jin, Xin Jiang,  Qun Liu, Michael Lyu, Irwin King<br>
       <i>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), Aug 2021. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a> GhostBERT: Generate More Features with Cheap Operations for BERT </a><br>
       Zhiqi Huang, <b>Lu Hou</b>, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu<br>
       <i>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), <b>Oral</b>, Aug 2021. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://openreview.net/pdf?id=9G5MIc-goqB"> Reweighting Augmented Samples by Minimizing the Maximal Expected Loss </a><br>
       Mingyang Yi, <b>Lu Hou</b>, Lifeng Shang, Xin Jiang, Qun Liu, Zhi-Ming Ma<br>
       <i>Proceedings of the Ninth International Conference on Learning Representations (ICLR), May 2021. </i> <br>
    </p>
  </li>
	
  <li>
    <p>
       <a href="https://proceedings.neurips.cc/paper/2020/hash/6f5216f8d89b086c18298e043bfe48ed-Abstract.html"> DynaBERT: Dynamic BERT with Adaptive Width and Depth </a>[<a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT">code</a>]<br>
       <b>Lu Hou</b>, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu<br>
       <i>Proceedings of the Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS), <b>Spotlight(4.07%)</b>, Dec 2020. </i> <br>
    </p>
  </li>
	
  <li>
    <p>
       <a href="https://arxiv.org/abs/2009.12812.pdf"> TernaryBERT: Distillation-aware Ultra-low Bit BERT </a>[<a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/ternarybert">pytorch code</a>][<a href="https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/nlp/ternarybert">mindspore code</a>]<br>
       Wei Zhang*, <b>Lu Hou*</b>, Yichun Yin*, Lifeng Shang, Xiao Chen, Xin Jiang, Qun Liu<br>
       <i>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Nov 2020. </i> <br>
    </p>
  </li>
	
  <li>
    <p>
       <a href="./papers/nips19.pdf"> Normalization Helps Training of Quantized LSTM</a> [<a href="https://github.com/houlu369/Normalized-Quantized-LSTM">code</a>]<br>
       <b>Lu Hou</b>, Jinhua Zhu, James T. Kwok, Fei Gao, Tao Qin, Tie-yan Liu<br>
       <i>Proceedings of the Thirty-third Conference on Neural Information Processing Systems (NeurIPS), Vancouver, BC, Canada, Dec 2019. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="./papers/iclr19.pdf"> Analysis of Quantized Models</a> <br>
       <b>Lu Hou</b>, Ruiliang Zhang, James T. Kwok<br>
       <i>Proceedings of the Seventh International Conference on Learning Representations (ICLR), New Orleans, Louisinna, USA, May 2019. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="./papers/iclr18.pdf"> Loss-aware Weight Quantization of Deep Networks</a> [<a href="https://github.com/houlu369/Loss-aware-weight-quantization">code</a>]<br>
       <b>Lu Hou</b>, James T. Kwok<br>
       <i>Proceedings of the Sixth International Conference on Learning Representations (ICLR), Vancouver, BC, Canada, Apr 2018. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="./papers/iclr17.pdf"> Loss-aware Binarization of Deep Networks</a> [<a href="https://github.com/houlu369/Loss-aware-Binarization">code</a>]<br>
       <b>Lu Hou</b>, Quanming Yao, James T. Kwok<br>
       <i>Proceedings of the Fifth International Conference on Learning Representations (ICLR), Toulon, France, Apr 2017. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="./papers/aaai16.pdf"> Efficient Learning of Timeseries Shapelets</a> [<a href="https://github.com/houlu369/FLAG_shapelets">code</a>]  <br>
       <b>Lu Hou</b>, James T. Kwok, Jacek M. Zurada<br>
       <i> Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI), pp.1209-1215, Phoenix, AZ, USA, Feb 2016. </i> <br>
    </p>
  </li>
</ol>
</div>


<div style="padding:6px;"> </div>

<h3>Working Experience</h3>
<ul>
  <li> Research Intern, Machine Learning Group, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/"> Microsoft Research Asia </a> <br>
    Beijing, China, Oct 2018 - March 2019 </li>
  <li> Student Intern, <a href="https://www.ri.cmu.edu/"> Robotics Institute, Carnegie Mellon University</a> <br>
    Pittsburgh, PA, USA, Jun 2013 - Aug 2013 </li>
</ul>

<div style="padding:6px;"> </div>


<h3>Awards</h3>
<ul>
 <li> Postgraduate Studentship, HKUST 2014-2019  </li>
 <li> Travel Award, AAAI 2016, ICLR 2017, ICLR 2018 </li>
 <li> Outstanding Graduate, NUST 2014 </li>
 <li> NUST Chancellor Medal (highest student award in NUST), NUST 2013 </li>
 <li> Chinese National Scholarship, NUST 2012 & 2013 </li>
 <li> Honorable Mention in Interdisciplinary Contest in Modeling (ICM) International Contest, 2013  </li>                                                                      
 <li> First Place in China Undergraduate Mathematical Contest in Modeling National Contest, 2012 </li>
</ul>

<div style="padding:6px;"> </div>
<h3>Academic Services</h3>
<ul>
 <li> <b>PC Member:</b> AAAI 2019-2022, IJCAI 2021-2022  </li>
 <li> <b>Conference Reviewer:</b> ICML 2018-2022, NeurIPS 2018-2022, ICLR 2019-2022, AISTATS 2019, ECCV 2020  </li>
 <li> <b>Journal Reviewer:</b> 
	<li> Machine Learning </li>
	<li> Artificial Intelligence </li>
	<li> IEEE Transactions on Cybernetics </li>
 	<li> IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) </li>
	<li> Neural Networks </li>
	<li> IEEE Transactions on Pattern Analysis and Machine Intelligence </li>
	</li>
	
</ul>



<div style="padding:6px;"> </div>

<h3>Teaching Experience</h3>
<ul>
  <li> COMP 5008 Introduction to Social Computing, Teaching Assistant, Spring 2017 & 2018</li>
  <li> COMP 4641 Social Information Networks Analysis and Engineering, Teaching Assistant, Spring 2017 & 2018 </li>
  <li> COMP 4331 Introduction to Data Mining, Teaching Assistant, Fall 2015, 2016 & 2017</li>
  <li> COMP 2011 Introduction to Object-Oriented Programming, Teaching Assistant, Spring 2015</li>
</ul>
</div>

  </div>
</div>


      </div>
      <footer>
         ©  2018 Lu Hou
        | <a href="http://www.cse.ust.hk/~lhouab/#top">To top <i class="icon-arrow-up"></i></a>
      </footer>

    </div> <!-- /container -->
  </div>






    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="./houlu_files/jquery.js.download"></script>
    <script src="./houlu_files/bootstrap-transition.js.download"></script>
    <script src="./houlu_files/bootstrap-alert.js.download"></script>
    <script src="./houlu_files/bootstrap-modal.js.download"></script>
    <script src="./houlu_files/bootstrap-dropdown.js.download"></script>
    <script src="./houlu_files/bootstrap-scrollspy.js.download"></script>
    <script src="./houlu_files/bootstrap-tab.js.download"></script>
    <script src="./houlu_files/bootstrap-tooltip.js.download"></script>
    <script src="./houlu_files/bootstrap-popover.js.download"></script>
    <script src="./houlu_files/bootstrap-button.js.download"></script>
    <script src="./houlu_files/bootstrap-collapse.js.download"></script>
    <script src="./houlu_files/bootstrap-carousel.js.download"></script>
    <script src="./houlu_files/bootstrap-typeahead.js.download"></script>










</body></html>
