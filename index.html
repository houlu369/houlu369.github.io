<!DOCTYPE html>
<!-- saved from url=(0032)http://www.cse.ust.hk/~lhouab/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lu Hou </title>
    <meta name="author" content="Lu Hou">

    <!-- Le styles -->
    <link href="./houlu_files/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="./houlu_files/font-awesome.min.css">
    <link href="./houlu_files/style.css" rel="stylesheet" type="text/css" media="all">
    <link rel="stylesheet" id="twentytwelve-style-css" target="_blank" href="./css/style2.css" type="text/css" media="all">
    <style type="text/css">
      body {
        padding-top: 30px;
        padding-bottom: 30px;
      }

      h3 {
        margin-top: 1.0em;
        margin-bottom: 0.3em;
        padding-bottom: 0.2em;
        line-height: 1.0;
        border-bottom: 1px solid #aaaaaa;
      }

      li {
        margin: 10px 0;
      }
    </style>


  </head>

  <body>

   

<div id="wrap">

<div class="container">

  <div class="content">


<div class="row">
  <div class="span14">


<div>



<div class="post-container">                
		<div class="post-thumb"><img src="./image/lulu.jpg" alt="protrait" width="180" style="margin-top:5px"></div>
		<div class="post-content">
			<h1 style="margin:-2px 0 0 0" class="civi_addr"> Lu Hou (‰æØÁíê)</h1>
					<p style="margin:-10px 0 0 0"class="civi_addr"> Researcher, Huawei Noah's Ark Lab </p>
					<p style="margin:-10px 0 0 0" class="civi_addr">Email: lhouab@connect.ust.hk</p>
					<p style="margin:3px 0 0 0" class="civi_addr">			
					I am a researcher at Huawei Noah's Ark Lab. I obtained my Ph.D. degree from <a href="https://www.ust.hk/"> Hong Kong University of Science and Technology </a> in 2019, under the supervision of Prof.<a href="https://www.cse.ust.hk/~jamesk/"> James T. Kwok</a>.  
Before that, I received my Bachelor's Degree from Qian Xuesen College (previously known as ‚ÄúCollege of Elite Education‚Äù) from Nanjing University of  Science and Technology </a> in 2014.</p>
			<b>[Hiring]</b> I am looking for research/engineering interns with strong machine learning and NLP/multimodal learning background. Please drop me an email if you are interested. 
	   <p style="margin:-10px 0 0 0" class="civi_addr">			
					   
	   </div>
</div>
		

<div style="padding:6px;"> </div>

<h3>Research Interests</h3>
<ul>
  <li>
    Multimodal Learning </li>
  <li>
    Compression and Acceleration of LLMs</li>
</ul>

<div style="padding:6px;"> </div>
<H3>News</H3>
  <UL>
  <LI>
  [2025-5] Congrats! One paper (<A href="https://arxiv.org/pdf/2410.09426">FlatQuant: Flatness Matters for LLM Quantization</A>) accepted to ICML 2025.  Code is available <A href="https://github.com/ruikangliu/FlatQuant"> here</A>.
  </LI>
</UL>
  
  <UL>
  <LI>
  [2025-4] I will serve as the Area Chair for NeurIPS 2025.
  </LI>
</UL>
  
  <UL>
  <LI>
  [2025-4]üî•Our preprint <A href="https://arxiv.org/abs/2504.04823v1"> "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models" </A> is now released and trending on <A href="https://www.alphaxiv.org/abs/2504.04823">alphaXiv</A>. Code will be available <A href="https://github.com/ruikangliu/Quantized-Reasoning-Models"> here</A>.
  </LI>
</UL>

<UL>
  <LI>
  [2025-4]üî•Our preprint <A href="https://arxiv.org/abs/2504.01934"> "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement" </A> is now released. Project Page is <A href="https://illume-unified-mllm.github.io/"> here</A>.
  </LI>
</UL>

  <UL>
  <LI>
  [2025-3]üî•Congrats! One paper (<A href="https://arxiv.org/pdf/2410.09426">EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions</A>) accepted to CVPR 2025.  Code on both Nvidia GPU and Ascend NPU is available <A href="https://github.com/emova-ollm/EMOVA"> here</A>.
  </LI>
</UL>
	


<div style="padding:6px;"> </div>
<h3>Selected Publications</h3>
<div>
(*: Equal contribution; #: Corresponding author/Project Leader; +: Mentor.)
<ol>

	
  <li>
    <p>
       <a href="https://arxiv.org/pdf/2410.09426">  EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions </a>[<a href="https://github.com/emova-ollm/EMOVA">code</a>]<br>
       Kai Chen<sup>*</sup>, Yunhao Gou<sup>*</sup>, Runhui Huang<sup>*</sup>, Zhili Liu<sup>*</sup>, Daxin Tan<sup>*</sup>, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong<sup>#</sup>, <b>Lu Hou</b><sup>#</sup>, Hang Xu<sup>#</sup><br>
       <i>Proceedings of IEEE / CVF Computer Vision and Pattern Recognition Conference 2025 (CVPR),  June 2025. </i> <br>
    </p>
  </li>

  <li>
    <p>
       <a href="">  TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding </a>[<a href="https://github.com/RenShuhuai-Andy/TimeChat">code</a>]<br>
       Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun,  <b>Lu Hou</b><sup>+</sup><br>
       <i>Proceedings of IEEE / CVF Computer Vision and Pattern Recognition Conference 2024 (CVPR),  June 2024. </i> <br>
    </p>
  </li>

		
  <li>
    <p>
       <a href="https://arxiv.org/abs/2212.09621.pdf">  Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding </a><br>
       Haoli Bai<sup>*</sup>, Zhiguang Liu<sup>*</sup>, Xiaojun Meng<sup>*</sup>, Wentao Li, Shuang Liu, Nian Xie, Rongfu Zheng, Liangwei Wang, <b>Lu Hou</b><sup>#</sup>, Jiansheng Wei, Xin Jiang, Qun Liu<br>
       <i>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL),  July 2023. </i> <br>
    </p>
  </li>
	
  <li>
    <p>
       <a> mCLIP: Multilingual CLIP via Cross-lingual Transfer </a>[<a href="https://github.com/ghchen18/acl23_mclip">code</a>]<br>
       Guanhua Chen, <b>Lu Hou</b><sup>+</sup>, Yun Chen, Wenliang Dai, Lifeng Shang, Xin Jiang, Qun Liu, Jia Pan and Wenping Wang<br>
       <i>To appear in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), <b>Oral</b>, July 2023. </i> <br>
    </p>
  </li>	
	
  <li>
    <p>
       <a> Structured Pruning for Efficient Generative Pre-trained Language Models </a><br>
       Chaofan Tao, <b>Lu Hou</b><sup>+</sup>, Haoli Bai<sup>+</sup>, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo and Ngai Wong <br>
       <i>Findings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL Findings), July 2023. </i> <br>
    </p>
  </li>
	
  <li>
    <p>
       <a href="https://arxiv.org/pdf/2210.11929.pdf"> LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling </a><br>
       Dongsheng Chen, Chaofan Tao, <b>Lu Hou</b><sup>+</sup>, Lifeng Shang, Xin Jiang, Qun Liu</br>
       <I> Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), Dec 2022. </i> <br>
    </p>
  </li>
	
	
  <li>
    <p>
       <a href="https://openreview.net/pdf?id=tvDRmAxGIjw">  Towards efficient post-training quantization of pre-trained language models </a><br>
       Haoli Bai, <b>Lu Hou</b><sup>+</sup>, Lifeng Shang, Xin Jiang, Irwin King, Michael R Lyu<br>
       <i> Proceedings of the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), Nov 2022. </i> <br>
    </p>
  </li>
	
   <li>
    <p>
       <a href="https://arxiv.org/pdf/2203.10705.pdf">  Compression of Generative Pre-trained Language Models via Quantization </a><br>
       Chaofan Tao, <b>Lu Hou</b><sup>+</sup>, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong<br>
       <i>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), <b>Outstanding Paper Award</b>, May 2022. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://openreview.net/pdf?id=8a9TvqmBNO7">  Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation </a><br>
       Wenliang Dai, <b>Lu Hou</b><sup>+</sup>, Lifeng Shang, Xin Jiang, Qun Liu, Pascale Fung<br>
       <i>Findings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL Findings), May 2022. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://openreview.net/pdf?id=cpDhcsEDC2"> FILIP: Fine-grained Interactive Language-Image Pre-Training </a>[<a href="https://wukong-dataset.github.io/wukong-dataset/download.html">checkpoints</a>]<br>
       Lewei Yao*, Runhui Huang*, <b>Lu Hou*</b>, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, Chunjing Xu<br>
       <i>Proceedings of the Tenth International Conference on Learning Representations (ICLR), Apr 2022. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://arxiv.org/pdf/2105.11144.pdf"> Improved OOD Generalization via Adversarial Training and Pretraining </a><br>
       Mingyang Yi, <b>Lu Hou</b><sup>+</sup><sup>#</sup>, Jiacheng Sun, Lifeng Shang, Xin Jiang,  Qun Liu, Zhiming Ma<br>
       <i>Proceedings of the Thirty-eighth International Conference on Machine Learning (ICML), Jul 2021. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://arxiv.org/pdf/2012.15701.pdf"> BinaryBERT: Pushing the Limit of BERT Quantization </a>[<a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BinaryBERT">code</a>]<br>
       Haoli Bai, Wei Zhang, <b>Lu Hou</b><sup>+</sup>, Lifeng Shang, Jing Jin, Xin Jiang,  Qun Liu, Michael Lyu, Irwin King<br>
       <i>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), <b>Accepted with scores 5, 5, 4</b>b. Aug 2021. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a> GhostBERT: Generate More Features with Cheap Operations for BERT </a><br>
       Zhiqi Huang, <b>Lu Hou</b><sup>+</sup>, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu<br>
       <i>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), <b>Oral</b>, Aug 2021. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://openreview.net/pdf?id=9G5MIc-goqB"> Reweighting Augmented Samples by Minimizing the Maximal Expected Loss </a><br>
       Mingyang Yi, <b>Lu Hou</b><sup>+</sup>, Lifeng Shang, Xin Jiang, Qun Liu, Zhi-Ming Ma<br>
       <i>Proceedings of the Ninth International Conference on Learning Representations (ICLR), May 2021. </i> <br>
    </p>
  </li>
	
  <li>
    <p>
       <a href="https://proceedings.neurips.cc/paper/2020/hash/6f5216f8d89b086c18298e043bfe48ed-Abstract.html"> DynaBERT: Dynamic BERT with Adaptive Width and Depth </a>[<a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT">code</a>]<br>
       <b>Lu Hou</b>, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu<br>
       <i>Proceedings of the Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS), <b>Spotlight(4.07%)</b>, Dec 2020. </i> <br>
    </p>
  </li>
	
  <li>
    <p>
       <a href="https://arxiv.org/abs/2009.12812.pdf"> TernaryBERT: Distillation-aware Ultra-low Bit BERT </a>[<a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/ternarybert">pytorch code</a>][<a href="https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/nlp/ternarybert">mindspore code</a>]<br>
       Wei Zhang*, <b>Lu Hou*</b>, Yichun Yin*, Lifeng Shang, Xiao Chen, Xin Jiang, Qun Liu<br>
       <i>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Nov 2020. </i> <br>
    </p>
  </li>
	
  <li>
    <p>
       <a href="./papers/nips19.pdf"> Normalization Helps Training of Quantized LSTM</a> [<a href="https://github.com/houlu369/Normalized-Quantized-LSTM">code</a>]<br>
       <b>Lu Hou</b>, Jinhua Zhu, James T. Kwok, Fei Gao, Tao Qin, Tie-yan Liu<br>
       <i>Proceedings of the Thirty-third Conference on Neural Information Processing Systems (NeurIPS), Vancouver, BC, Canada, Dec 2019. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="./papers/iclr19.pdf"> Analysis of Quantized Models</a> <br>
       <b>Lu Hou</b>, Ruiliang Zhang, James T. Kwok<br>
       <i>Proceedings of the Seventh International Conference on Learning Representations (ICLR), New Orleans, Louisinna, USA, May 2019. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="./papers/iclr18.pdf"> Loss-aware Weight Quantization of Deep Networks</a> [<a href="https://github.com/houlu369/Loss-aware-weight-quantization">code</a>]<br>
       <b>Lu Hou</b>, James T. Kwok<br>
       <i>Proceedings of the Sixth International Conference on Learning Representations (ICLR), Vancouver, BC, Canada, Apr 2018. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="./papers/iclr17.pdf"> Loss-aware Binarization of Deep Networks</a> [<a href="https://github.com/houlu369/Loss-aware-Binarization">code</a>]<br>
       <b>Lu Hou</b>, Quanming Yao, James T. Kwok<br>
       <i>Proceedings of the Fifth International Conference on Learning Representations (ICLR), Toulon, France, Apr 2017. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="./papers/aaai16.pdf"> Efficient Learning of Timeseries Shapelets</a> [<a href="https://github.com/houlu369/FLAG_shapelets">code</a>]  <br>
       <b>Lu Hou</b>, James T. Kwok, Jacek M. Zurada<br>
       <i> Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI), pp.1209-1215, Phoenix, AZ, USA, Feb 2016. </i> <br>
    </p>
  </li>
</ol>
</div>


<div style="padding:6px;"> </div>

<h3>Working Experience</h3>
<ul>
  <li> Research Intern, Machine Learning Group, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/"> Microsoft Research Asia </a> <br>
    Beijing, China, Oct 2018 - March 2019 </li>
  <li> Student Intern, <a href="https://www.ri.cmu.edu/"> Robotics Institute, Carnegie Mellon University</a> <br>
    Pittsburgh, PA, USA, Jun 2013 - Aug 2013 </li>
</ul>

<div style="padding:6px;"> </div>


<h3>Awards</h3>
<ul>
 <li> Postgraduate Studentship, HKUST 2014-2019  </li>
 <li> Travel Award, AAAI 2016, ICLR 2017, ICLR 2018 </li>
 <li> Outstanding Graduate, NUST 2014 </li>
 <li> NUST Chancellor Medal (highest student award in NUST), NUST 2013 </li>
 <li> Chinese National Scholarship, NUST 2012 & 2013 </li>
 <li> Honorable Mention in Interdisciplinary Contest in Modeling (ICM) International Contest, 2013  </li>                                                                      
 <li> First Place in China Undergraduate Mathematical Contest in Modeling National Contest, 2012 </li>
</ul>

<div style="padding:6px;"> </div>
<h3>Academic Services</h3>
<ul>
 <li> <b>Area Chair:</b> NeurIPS 2025  </li>
 <li> <b>PC Member/Reviewer:</b>   ICLR 2019-2025, NeurIPS 2018-2025, ICCV 2025, CVPR 2025, ACL 2025, ICML 2018-2022  </li>
 <li> <b>Journal Reviewer:</b> Machine Learning, Artificial Intelligence, IEEE Transactions on Pattern Analysis and Machine Intelligence </li>
	
</ul>



<div style="padding:6px;"> </div>

<h3>Teaching Experience</h3>
<ul>
  <li> COMP 5008 Introduction to Social Computing, Teaching Assistant, Spring 2017 & 2018</li>
  <li> COMP 4641 Social Information Networks Analysis and Engineering, Teaching Assistant, Spring 2017 & 2018 </li>
  <li> COMP 4331 Introduction to Data Mining, Teaching Assistant, Fall 2015, 2016 & 2017</li>
  <li> COMP 2011 Introduction to Object-Oriented Programming, Teaching Assistant, Spring 2015</li>
</ul>
</div>

  </div>
</div>


      </div>
      <footer>
         ¬©  2018 Lu Hou
        | <a href="http://www.cse.ust.hk/~lhouab/#top">To top <i class="icon-arrow-up"></i></a>
      </footer>

    </div> <!-- /container -->
  </div>






    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="./houlu_files/jquery.js.download"></script>
    <script src="./houlu_files/bootstrap-transition.js.download"></script>
    <script src="./houlu_files/bootstrap-alert.js.download"></script>
    <script src="./houlu_files/bootstrap-modal.js.download"></script>
    <script src="./houlu_files/bootstrap-dropdown.js.download"></script>
    <script src="./houlu_files/bootstrap-scrollspy.js.download"></script>
    <script src="./houlu_files/bootstrap-tab.js.download"></script>
    <script src="./houlu_files/bootstrap-tooltip.js.download"></script>
    <script src="./houlu_files/bootstrap-popover.js.download"></script>
    <script src="./houlu_files/bootstrap-button.js.download"></script>
    <script src="./houlu_files/bootstrap-collapse.js.download"></script>
    <script src="./houlu_files/bootstrap-carousel.js.download"></script>
    <script src="./houlu_files/bootstrap-typeahead.js.download"></script>










</body></html>
